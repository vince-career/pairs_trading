{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from config import config\n",
    "from rl_functions import plot_loss\n",
    "from rl_trading_framework import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e83ef5da6b450bb6771d954821bbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Getting data:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_data_all = get_data_torch(\n",
    "    config['tickers'], config['train_start'], config['train_end'], \n",
    "    config['start_time'], config['end_time'], config['multiplier'], \n",
    "    config['data_freq'], config['device'])\n",
    "\n",
    "# print(n_of_gpu, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Episodes: 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rolling training:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop at episode: 8 retained pairs after drop: 4866 temperature: 1.8 retain_threshold: 0.28\n",
      "drop at episode: 12 retained pairs after drop: 4566 temperature: 1.6 retain_threshold: 0.36000000000000004\n",
      "drop at episode: 16 retained pairs after drop: 3930 temperature: 1.4000000000000001 retain_threshold: 0.44000000000000006\n",
      "drop at episode: 20 retained pairs after drop: 2330 temperature: 1.2000000000000002 retain_threshold: 0.52\n",
      "drop at episode: 24 retained pairs after drop: 1020 temperature: 1.0000000000000002 retain_threshold: 0.6\n",
      "drop at episode: 28 retained pairs after drop: 288 temperature: 0.8000000000000003 retain_threshold: 0.6799999999999999\n",
      "drop at episode: 32 retained pairs after drop: 94 temperature: 0.6000000000000003 retain_threshold: 0.7599999999999999\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "train(0, 0, price_data_all, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, n = price_data_all.shape\n",
    "all_pairs = list(combinations(range(n), 2))\n",
    "\n",
    "if full_sample_train:\n",
    "    retained_pairs = generate_pairs(all_pairs, n_of_gpu, full_sample=full_sample_train)\n",
    "else:\n",
    "    retained_pairs = generate_pairs(all_pairs, n_of_gpu, k=batch_size_train)\n",
    "\n",
    "batch_size_train = len(retained_pairs)\n",
    "print('Total time steps:', t)\n",
    "print('Total number of individual assets:', n)\n",
    "print('Total number of possible pairs:', len(all_pairs))\n",
    "print('Current training uses', batch_size_train, 'pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_encoding_model = StateEncodingModel(device, input_dim=8, lstm_hidden_dim=lstm_hidden_dim, num_layers=lstm_num_layers).to(device)\n",
    "trading_policy_model = TradingPolicyModel(lstm_hidden_dim, mlp_hidden_dim, action_dim).to(device)\n",
    "trading_agent = TradingAgent(state_encoding_model, trading_policy_model, optimizer_class, learning_rate)\n",
    "env = TradingEnvironment(state_encoding_model, reg_rolling_window, portfolio_settings, action_dim, all_pairs, n_of_gpu)\n",
    "\n",
    "if n_of_gpu > 1:\n",
    "    state_encoding_model = nn.DataParallel(state_encoding_model)\n",
    "    trading_policy_model = nn.DataParallel(trading_policy_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_threshold = initial_retain_threshold\n",
    "temperature = initial_T\n",
    "# episodes_per_rolling = initial_episodes_per_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see updated loss and reward every episode, uncomment the next two lines and plot_loss(fig, ax, dh, ...)\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(12, 8))     # display the initial figure and get a display handle, outside the main loop\n",
    "dh = display.display(fig, display_id=True)\n",
    "\n",
    "total_episodes = len(range(0, price_data_all.shape[0] - rolling_window_size, rolling_step_size)) * episodes_per_rolling\n",
    "print('Total Episodes:', total_episodes)\n",
    "\n",
    "drop_start = int(total_episodes * drop_start_ratio)\n",
    "drop_end = int(total_episodes * drop_end_ratio)\n",
    "next_drop = drop_start\n",
    "\n",
    "episode_count = 0\n",
    "retained_pairs_len = []\n",
    "start_t = time.time()\n",
    "\n",
    "for start_idx in tqdm(range(0, price_data_all.shape[0]-rolling_window_size, rolling_step_size), desc=\"Rolling training\"):\n",
    "    \n",
    "    price_data = price_data_all[start_idx:start_idx+rolling_window_size, :]\n",
    "    env.update_data(price_data)\n",
    "    \n",
    "    for episode in range(episodes_per_rolling):\n",
    "\n",
    "        episode_count += 1\n",
    "        # todo: check epsilon-greedy\n",
    "        if episode_count <= total_episodes * exploration_fraction:\n",
    "            method = 'random'\n",
    "        else:\n",
    "            method = 'greedy'\n",
    "            \n",
    "#         temperature = initial_T * (T_decay_rate ** episode_count)\n",
    "#         retain_threshold = initial_retain_threshold * (retain_threshold_growth_rate ** episode_count)\n",
    "#         retain_threshold = initial_retain_threshold + 0.01 * episode_count\n",
    "\n",
    "        batch_size_train = len(retained_pairs)\n",
    "        retained_pairs_len.append(batch_size_train)\n",
    "            \n",
    "        # 1. reset model and env, initiate observation\n",
    "        if isinstance(state_encoding_model, nn.DataParallel):\n",
    "            state_encoding_model.module.reset_state(batch_size_train//n_of_gpu, state_encoding_model)\n",
    "        else:\n",
    "            state_encoding_model.reset_state(batch_size_train, state_encoding_model)\n",
    "            \n",
    "        trading_agent.reset()\n",
    "        env.reset(pair_indices_given=retained_pairs)\n",
    "        ob_t = env.update_observation()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        \n",
    "        while not done:\n",
    "            # 2. lstm decides a state\n",
    "\n",
    "            encoded_state = state_encoding_model(ob_t)\n",
    "            env.norm_port_value = (env.port_value / env.bt_settings['initial_cash']).unsqueeze(1)\n",
    "            # unsqueeze dim from (batch_size_train) to (batch_size_train, 1) for torch.cat\n",
    "\n",
    "            # 3. agent makes a decision\n",
    "            action = trading_agent.choose_action(encoded_state, env.norm_port_value, method)\n",
    "            # action is the desired position at time t\n",
    "\n",
    "            # 4. calculate reward, update observations based on action\n",
    "            reward, ob_t, done = env.step(action)\n",
    "            trading_agent.rewards.append(reward)\n",
    "\n",
    "            # 5. store state, action, reward\n",
    "            trajectory.append((encoded_state, action, reward))\n",
    "            \n",
    "        # 6. learn after getting the whole trajectory\n",
    "        trading_agent.rewards = torch.stack(trading_agent.rewards)\n",
    "        trading_agent.learn(gamma=0.99)\n",
    "        plot_loss(fig, ax1, dh, trading_agent.loss_history_all)\n",
    "        plot_mean_reward(fig, ax2, dh, trading_agent.mean_reward_all)\n",
    "#         plot_batch_size(fig, ax3, dh, retained_pairs_len)\n",
    "        \n",
    "        # 7. filter pairs\n",
    "        if drop_start <= episode_count <= drop_end and episode_count >= next_drop and len(retained_pairs) > stop_drop_threshold:\n",
    "            \n",
    "            retained_pairs = trading_agent.retain_pairs(retained_pairs, alpha, beta, temperature, retain_threshold, adjust_for_n_gpu=True, n_of_gpu=n_of_gpu)\n",
    "            next_drop = episode_count + int(total_episodes * drop_pairs_percentage_interval)\n",
    "            temperature -= 0.2\n",
    "            retain_threshold += 0.08\n",
    "#             episodes_per_rolling += 10\n",
    "            print('drop at episode:', episode_count, 'retained pairs after drop:', len(retained_pairs), 'temperature:', temperature, 'retain_threshold:', retain_threshold)\n",
    "\n",
    "end_t = time.time()\n",
    "print('Total training time:', round(end_t - start_t, 2), 'seconds')\n",
    "           \n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(12, 24))\n",
    "\n",
    "ax[0].plot(trading_agent.loss_history_all)\n",
    "ax[0].set_title(\"Training Loss over Time\")\n",
    "ax[0].set_xlabel(\"Episode Step\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "\n",
    "ax[1].plot(trading_agent.mean_reward_all)\n",
    "ax[1].set_title(\"Mean Rewards over Time\")\n",
    "ax[1].set_xlabel(\"Episode Step\")\n",
    "ax[1].set_ylabel(\"Mean Rewards\")\n",
    "\n",
    "ax[2].plot(retained_pairs_len)\n",
    "ax[2].set_title(\"Number of Pairs Retained over Time\")\n",
    "ax[2].set_xlabel(\"Episode Step\")\n",
    "ax[2].set_ylabel(\"Number of Pairs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "save_model(state_encoding_model, model_folder_path, encoding_model_file_name)\n",
    "save_model(trading_policy_model, model_folder_path, policy_model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retained_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pair_names = [f\"{tickers[i]}-{tickers[j]}\" for i, j in retained_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pair_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store selected_pair_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
